@article{p137,
  journal = {IEEE TVCG},
  year = 2015,
  title = {Temporal MDS Plots for Analysis of Multivariate Data},
  doi = {10.1109/TVCG.2015.2467553},
  url = {http://dx.doi.org/10.1109/TVCG.2015.2467553},
  author = {JÃ¤ckle, D. and Fischer, F. and Schreck, T. and Keim, D.A.},
  pages = {141--150},
  keywords = {Multivariate Data, Time Series, Data Reduction, Multidimensional Scaling},
  abstract = {Multivariate time series data can be found in many application domains. Examples include data from computer networks, healthcare, social networks, or financial markets. Often, patterns in such data evolve over time among multiple dimensions and are hard to detect. Dimensionality reduction methods such as PCA and MDS allow analysis and visualization of multivariate data, but per se do not provide means to explore multivariate patterns over time. We propose Temporal Multidimensional Scaling (TMDS), a novel visualization technique that computes temporal one-dimensional MDS plots for multivariate data which evolve over time. Using a sliding window approach, MDS is computed for each data window separately, and the results are plotted sequentially along the time axis, taking care of plot alignment. Our TMDS plots enable visual identification of patterns based on multidimensional similarity of the data evolving over time. We demonstrate the usefulness of our approach in the field of network security and show in two case studies how users can iteratively explore the data to identify previously unknown, temporally evolving patterns.},
}
@article{p139,
  journal = {IEEE TVCG},
  year = 2015,
  title = {The Role of Uncertainty, Awareness, and Trust in Visual Analytics},
  doi = {10.1109/TVCG.2015.2467591},
  url = {http://dx.doi.org/10.1109/TVCG.2015.2467591},
  author = {Sacha, D. and Senaratne, H. and Bum Chul Kwon and Ellis, G. and Keim, D.A.},
  pages = {240--249},
  keywords = {Visual Analytics, Knowledge Generation, Uncertainty Measures and Propagation, Trust Building, Human Factors},
  abstract = {Visual analytics supports humans in generating knowledge from large and often complex datasets. Evidence is collected, collated and cross-linked with our existing knowledge. In the process, a myriad of analytical and visualisation techniques are employed to generate a visual representation of the data. These often introduce their own uncertainties, in addition to the ones inherent in the data, and these propagated and compounded uncertainties can result in impaired decision making. The user's confidence or trust in the results depends on the extent of user's awareness of the underlying uncertainties generated on the system side. This paper unpacks the uncertainties that propagate through visual analytics systems, illustrates how human's perceptual and cognitive biases influence the user's awareness of such uncertainties, and how this affects the user's trust building. The knowledge generation model for visual analytics is used to provide a terminology and framework to discuss the consequences of these aspects in knowledge construction and though examples, machine uncertainty is compared to human trust measures with provenance. Furthermore, guidelines for the design of uncertainty-aware systems are presented that can aid the user in better decision making.},
}
@inproceedings{p256,
  booktitle = {Proc. VAST},
  year = 2014,
  title = {Feature-Driven Visual Analytics of Soccer Data},
  doi = {10.1109/VAST.2014.7042477},
  url = {http://dx.doi.org/10.1109/VAST.2014.7042477},
  author = {Janetzko, H. and Sacha, D. and Stein, M. and Schreck, T. and Deussen, O. and Keim, D.A.},
  pages = {13--22},
  keywords = {Visual Analytics, Sport Analytics, Soccer Analysis},
  abstract = {Soccer is one the most popular sports today and also very interesting from an scientific point of view. We present a system for analyzing high-frequency position-based soccer data at various levels of detail, allowing to interactively explore and analyze for movement features and game events. Our Visual Analytics method covers single-player, multi-player and event-based analytical views. Depending on the task the most promising features are semi-automatically selected, processed, and visualized. Our aim is to help soccer analysts in finding the most important and interesting events in a match. We present a flexible, modular, and expandable layer-based system allowing in-depth analysis. The integration of Visual Analytics techniques into the analysis process enables the analyst to find interesting events based on classification and allows, by a set of custom views, to communicate the found results. The feedback loop in the Visual Analytics pipeline helps to further improve the classification results. We evaluate our approach by investigating real-world soccer matches and collecting additional expert feedback. Several use cases and findings illustrate the capabilities of our approach.},
}
@article{p265,
  journal = {IEEE TVCG},
  year = 2014,
  title = {Knowledge Generation Model for Visual Analytics},
  doi = {10.1109/TVCG.2014.2346481},
  url = {http://dx.doi.org/10.1109/TVCG.2014.2346481},
  author = {Sacha, D. and Stoffel, A. and Stoffel, F. and Bum Chul Kwon and Ellis, G. and Keim, D.A.},
  pages = {1604--1613},
  keywords = {Visual Analytics, Knowledge Generation, Reasoning, Visualization Taxonomies and Models, Interaction},
  abstract = {Visual analytics enables us to analyze huge information spaces in order to support complex decision making and data exploration. Humans play a central role in generating knowledge from the snippets of evidence emerging from visual data analysis. Although prior research provides frameworks that generalize this process, their scope is often narrowly focused so they do not encompass different perspectives at different levels. This paper proposes a knowledge generation model for visual analytics that ties together these diverse frameworks, yet retains previously developed models (e.g., KDD process) to describe individual segments of the overall visual analytic processes. To test its utility, a real world visual analytics system is compared against the model, demonstrating that the knowledge generation process model provides a useful guideline when developing and evaluating such systems. The model is used to effectively compare different data analysis systems. Furthermore, the model provides a common language and description of visual analytic processes, which can be used for communication between researchers. At the end, our model reflects areas of research that future researchers can embark on.},
}
@misc{p456,
  year = 2012,
  title = {Exploring cyber physical data streams using Radial Pixel Visualizations},
  doi = {10.1109/VAST.2012.6400541},
  url = {http://dx.doi.org/10.1109/VAST.2012.6400541},
  author = {Hao, M.C. and Marwah, M. and Mittelstadt, S. and Janetzko, H. and Keim, D.A. and Dayal, U. and Bash, C. and Felix, C. and Patel, C. and Hsu, M. and Chen, Y.},
  pages = {225--226},
  keywords = {},
  abstract = {Cyber physical systems (CPS), such as smart buildings and data centers, are richly instrumented systems composed of tightly coupled computational and physical elements that generate large amounts of data. To explore CPS data and obtain actionable insights, we construct a Radial Pixel Visualization (RPV) system, which uses multiple concentric rings to show the data in a compact circular layout of small polygons (pixel cells), each of which represents an individual data value. RPV provides an effective visual representation of locality and periodicity of the high volume, multivariate data streams, and seamlessly combines them with the results of an automated analysis. In the outermost ring the results of correlation analysis and peak point detection are highlighted. Our explorations demonstrates how RPV can help administrators to identify periodic thermal hot spots, understand data center energy consumption, and optimize IT workload.},
}
@misc{p467,
  year = 2012,
  title = {Matrix-based visual correlation analysis on large timeseries data},
  doi = {10.1109/VAST.2012.6400549},
  url = {http://dx.doi.org/10.1109/VAST.2012.6400549},
  author = {Behrisch, M. and Davey, J. and Schreck, T. and Keim, D.A. and Kohlhammer, J.},
  pages = {209--210},
  keywords = {},
  abstract = {In recent years, the quantity of time series data generated in a wide variety of domains grown consistently. Thus, it is difficult for analysts to process and understand this overwhelming amount of data. In the specific case of time series data another problem arises: time series can be highly interrelated. This problem becomes even more challenging when a set of parameters influences the progression of a time series. However, while most visual analysis techniques support the analysis of short time periods, e.g. one day or one week, they fail to visualize large-scale time series, ranging over one year or more. In our approach we present a time series matrix visualization that tackles this problem. Its primary advantages are that it scales to a large number of time series with different start and end points and allows for the visual comparison / correlation analysis of a set of influencing factors. To evaluate our approach, we applied our technique to a real-world data set, showing the impact of local weather conditions on the efficiency of photovoltaic power plants.},
}
@inproceedings{p477,
  booktitle = {Proc. VAST},
  year = 2012,
  title = {Subspace search and visualization to make sense of alternative clusterings in high-dimensional data},
  doi = {10.1109/VAST.2012.6400488},
  url = {http://dx.doi.org/10.1109/VAST.2012.6400488},
  author = {Tatu, A. and Maas, F. and Farber, I. and Bertini, E. and Schreck, T. and Seidl, T. and Keim, D.A.},
  pages = {63--72},
  keywords = {},
  abstract = {In explorative data analysis, the data under consideration often resides in a high-dimensional (HD) data space. Currently many methods are available to analyze this type of data. So far, proposed automatic approaches include dimensionality reduction and cluster analysis, whereby visual-interactive methods aim to provide effective visual mappings to show, relate, and navigate HD data. Furthermore, almost all of these methods conduct the analysis from a singular perspective, meaning that they consider the data in either the original HD data space, or a reduced version thereof. Additionally, HD data spaces often consist of combined features that measure different properties, in which case the particular relationships between the various properties may not be clear to the analysts a priori since it can only be revealed if appropriate feature combinations (subspaces) of the data are taken into consideration. Considering just a single subspace is, however, often not sufficient since different subspaces may show complementary, conjointly, or contradicting relations between data items. Useful information may consequently remain embedded in sets of subspaces of a given HD input data space. Relying on the notion of subspaces, we propose a novel method for the visual analysis of HD data in which we employ an interestingness-guided subspace search algorithm to detect a candidate set of subspaces. Based on appropriately defined subspace similarity functions, we visualize the subspaces and provide navigation facilities to interactively explore large sets of subspaces. Our approach allows users to effectively compare and relate subspaces with respect to involved dimensions and clusters of objects. We apply our approach to synthetic and real data sets. We thereby demonstrate its support for understanding HD data from different perspectives, effectively yielding a more complete view on HD data.},
}
@inproceedings{p485,
  booktitle = {Proc. VAST},
  year = 2012,
  title = {Visual analytics for the big data era---A comparative review of state-of-the-art commercial systems},
  doi = {10.1109/VAST.2012.6400554},
  url = {http://dx.doi.org/10.1109/VAST.2012.6400554},
  author = {Leishi Zhang and Stoffel, A. and Behrisch, M. and Mittelstadt, S. and Schreck, T. and Pompl, R. and Weber, S. and Last, H. and Keim, D.A.},
  pages = {173--182},
  keywords = {},
  abstract = {Visual analytics (VA) system development started in academic research institutions where novel visualization techniques and open source toolkits were developed. Simultaneously, small software companies, sometimes spin-offs from academic research institutions, built solutions for specific application domains. In recent years we observed the following trend: some small VA companies grew exponentially; at the same time some big software vendors such as IBM and SAP started to acquire successful VA companies and integrated the acquired VA components into their existing frameworks. Generally the application domains of VA systems have broadened substantially. This phenomenon is driven by the generation of more and more data of high volume and complexity, which leads to an increasing demand for VA solutions from many application domains. In this paper we survey a selection of state-of-the-art commercial VA frameworks, complementary to an existing survey on open source VA tools. From the survey results we identify several improvement opportunities as future research directions.},
}
@article{p546,
  journal = {IEEE TVCG},
  year = 2011,
  title = {CloudLines: Compact Display of Event Episodes in Multiple Time-Series},
  doi = {10.1109/TVCG.2011.179},
  url = {http://dx.doi.org/10.1109/TVCG.2011.179},
  author = {Krstajic, M. and Bertini, E. and Keim, D.A.},
  pages = {2432--2439},
  keywords = {Incremental Visualization, Event-based Data, Lens Distortion},
  abstract = {We propose incremental logarithmic time-series technique as a way to deal with time-based representations of large and dynamic event data sets in limited space. Modern data visualization problems in the domains of news analysis, network security and financial applications, require visual analysis of incremental data, which poses specific challenges that are normally not solved by static visualizations. The incremental nature of the data implies that visualizations have to necessarily change their content and still provide comprehensible representations. In particular, in this paper we deal with the need to keep an eye on recent events together with providing a context on the past and to make relevant patterns accessible at any scale. Our technique adapts to the incoming data by taking care of the rate at which data items occur and by using a decay function to let the items fade away according to their relevance. Since access to details is also important, we also provide a novel distortion magnifying lens technique which takes into account the distortions introduced by the logarithmic time scale to augment readability in selected areas of interest. We demonstrate the validity of our techniques by applying them on incremental data coming from online news streams in different time frames.},
}
@article{p572,
  journal = {IEEE TVCG},
  year = 2011,
  title = {Quality Metrics in High-Dimensional Data Visualization: An Overview and Systematization},
  doi = {10.1109/TVCG.2011.229},
  url = {http://dx.doi.org/10.1109/TVCG.2011.229},
  author = {Bertini, E. and Tatu, A. and Keim, D.A.},
  pages = {2203--2212},
  keywords = {Quality Metrics, High-Dimensional Data Visualization},
  abstract = {In this paper, we present a systematization of techniques that use quality metrics to help in the visual exploration of meaningful patterns in high-dimensional data. In a number of recent papers, different quality metrics are proposed to automate the demanding search through large spaces of alternative visualizations (e.g., alternative projections or ordering), allowing the user to concentrate on the most promising visualizations suggested by the quality metrics. Over the last decade, this approach has witnessed a remarkable development but few reflections exist on how these methods are related to each other and how the approach can be developed further. For this purpose, we provide an overview of approaches that use quality metrics in high-dimensional data visualization and propose a systematization based on a thorough literature review. We carefully analyze the papers and derive a set of factors for discriminating the quality metrics, visualization techniques, and the process itself. The process is described through a reworked version of the well-known information visualization pipeline. We demonstrate the usefulness of our model by applying it to several existing approaches that use quality metrics, and we provide reflections on implications of our model for future research.},
}
@misc{p643,
  year = 2011,
  title = {Visual analytics of terrorist activities related to epidemics},
  doi = {10.1109/VAST.2011.6102498},
  url = {http://dx.doi.org/10.1109/VAST.2011.6102498},
  author = {Bertini, E. and Buchmuller, J. and Fischer, F. and Huber, S. and Lindemeier, T. and Maass, F. and Mansmann, F. and Ramm, T. and Regenscheit, M. and Rohrdantz, C. and Scheible, C. and Schreck, T. and Sellien, S. and Stoffel, F. and Tautzenberger, M. and Zieker, M. and Keim, D.A.},
  pages = {329--330},
  keywords = {},
  abstract = {The task of the VAST 2011 Grand Challenge was to investigate potential terrorist activities and their relation to the spread of an epidemic. Three different data sets were provided as part of three Mini Challenges (MCs). MC 1 was about analyzing geo-tagged microblogging (Twitter) messages to characterize the spread of an epidemic. MC 2 required analyzing threats to a computer network using a situational awareness approach. In MC 3 possible criminal and terrorist activities were to be analyzed based on a collection of news articles. To solve the Grand Challenge, insight from each of the individual MCs had to be integrated appropriately.},
}
@misc{p644,
  year = 2011,
  title = {Visual sentiment analysis on twitter data streams},
  doi = {10.1109/VAST.2011.6102472},
  url = {http://dx.doi.org/10.1109/VAST.2011.6102472},
  author = {Ming Hao and Rohrdantz, C. and Janetzko, H. and Dayal, U. and Keim, D.A. and Haug, L. and Mei-Chun Hsu},
  pages = {277--278},
  keywords = {},
  abstract = {Twitter currently receives about 190 million tweets (small text-based Web posts) a day, in which people share their comments regarding a wide range of topics. A large number of tweets include opinions about products and services. However, with Twitter being a relatively new phenomenon, these tweets are underutilized as a source for evaluating customer sentiment. To explore high-volume twitter data, we introduce three novel time-based visual sentiment analysis techniques: (1) topic-based sentiment analysis that extracts, maps, and measures customer opinions; (2) stream analysis that identifies interesting tweets based on their density, negativity, and influence characteristics; and (3) pixel cell-based sentiment calendars and high density geo maps that visualize large volumes of data in a single view. We applied these techniques to a variety of twitter data, (e.g., movies, amusement parks, and hotels) to show their distribution and patterns, and to identify influential opinions.},
}
@misc{p783,
  year = 2010,
  title = {Visual analysis of frequent patterns in large time series},
  doi = {10.1109/VAST.2010.5650766},
  url = {http://dx.doi.org/10.1109/VAST.2010.5650766},
  author = {Hao, M.C. and Marwah, M. and Janetzko, H. and Keim, D.A. and Dayal, U. and Sharma, R. and Patnaik, D. and Ramakrishnan, N.},
  pages = {227--228},
  keywords = {},
  abstract = {The detection of previously unknown, frequently occurring patterns in time series, often called motifs, has been recognized as an important task. To find these motifs, we use an advanced temporal data mining algorithm. Since our algorithm usually finds hundreds of motifs, we need to analyze and access the discovered motifs. For this purpose, we introduce three novel visual analytics methods: (1) motif layout, using colored rectangles for visualizing the occurrences and hierarchical relationships of motifs in a multivariate time series, (2) motif distortion, for enlarging or shrinking motifs as appropriate for easy analysis and (3) motif merging, to combine a number of identical adjacent motif instances without cluttering the display. We have applied and evaluated our methods using two real-world data sets: data center cooling and oil well production.},
}
@inproceedings{p787,
  booktitle = {Proc. VAST},
  year = 2010,
  title = {Visual market sector analysis for financial time series data},
  doi = {10.1109/VAST.2010.5652530},
  url = {http://dx.doi.org/10.1109/VAST.2010.5652530},
  author = {Ziegler, H. and Jenny, M. and Gruse, T. and Keim, D.A.},
  pages = {83--90},
  keywords = {Visual Analytics, financial Information Visualization, Time Series Data, Time Series Clustering, Explorative Analysis\n\n},
  abstract = {The massive amount of financial time series data that originates from the stock market generates large amounts of complex data of high interest. However, adequate solutions that can effectively handle the information in order to gain insight and to understand the market mechanisms are rare. In this paper, we present two techniques and applications that enable the user to interactively analyze large amounts of time series data in real-time in order to get insight into the development of assets, market sectors, countries, and the financial market as a whole. The first technique allows users to quickly analyze combinations of single assets, market sectors as well as countries, compare them to each other, and to visually discover the periods of time where market sectors and countries get into turbulence. The second application clusters a selection of large amounts of financial time series data according to their similarity, and analyzes the distribution of the assets among market sectors. This allows users to identify the characteristic graphs which are representative for the development of a particular market sector, and also to identify the assets which behave considerably differently compared to other assets in the same sector. Both applications allow the user to perform investigative exploration techniques and interactive visual analysis in real-time.},
}
@inproceedings{p788,
  booktitle = {Proc. VAST},
  year = 2010,
  title = {Visual readability analysis: How to make your writings easier to read},
  doi = {10.1109/VAST.2010.5652926},
  url = {http://dx.doi.org/10.1109/VAST.2010.5652926},
  author = {Oelke, D. and Spretke, D. and Stoffel, A. and Keim, D.A.},
  pages = {123--130},
  keywords = {},
  abstract = {We present a tool that is specifically designed to support a writer in revising a draft-version of a document. In addition to showing which paragraphs and sentences are difficult to read and understand, we assist the reader in understanding why this is the case. This requires features that are expressive predictors of readability, and are also semantically understandable. In the first part of the paper, we therefore discuss a semi-automatic feature selection approach that is used to choose appropriate measures from a collection of 141 candidate readability features. In the second part, we present the visual analysis tool VisRA, which allows the user to analyze the feature values across the text and within single sentences. The user can choose different visual representations accounting for differences in the size of the documents and the availability of information about the physical and logical layout of the documents. We put special emphasis on providing as much transparency as possible to ensure that the user can purposefully improve the readability of a sentence. Several case-studies are presented that show the wide range of applicability of our tool.},
}
@article{p854,
  journal = {IEEE TVCG},
  year = 2009,
  title = {Document Cards: A Top Trumps Visualization for Documents},
  doi = {10.1109/TVCG.2009.139},
  url = {http://dx.doi.org/10.1109/TVCG.2009.139},
  author = {Strobelt, H. and Oelke, D. and Rohrdantz, C. and Stoffel, A. and Keim, D.A. and Deussen, O.},
  pages = {1145--1152},
  keywords = {document visualization, visual summary, content extraction, document collection browsing},
  abstract = {Finding suitable, less space consuming views for a document's main content is crucial to provide convenient access to large document collections on display devices of different size. We present a novel compact visualization which represents the document's key semantic as a mixture of images and important key terms, similar to cards in a top trumps game. The key terms are extracted using an advanced text mining approach based on a fully automatic document structure extraction. The images and their captions are extracted using a graphical heuristic and the captions are used for a semi-semantic image weighting. Furthermore, we use the image color histogram for classification and show at least one representative from each non-empty image class. The approach is demonstrated for the IEEE InfoVis publications of a complete year. The method can easily be applied to other publication collections and sets of documents which contain images.},
}
@article{p871,
  journal = {IEEE TVCG},
  year = 2009,
  title = {Spatiotemporal Analysis of Sensor Logs using Growth Ring Maps},
  doi = {10.1109/TVCG.2009.182},
  url = {http://dx.doi.org/10.1109/TVCG.2009.182},
  author = {Bak, P. and Mansmann, F. and Janetzko, H. and Keim, D.A.},
  pages = {913--920},
  keywords = {spatiotemporal visualization, visual analytics, animal behavior, dense pixel displays},
  abstract = {Spatiotemporal analysis of sensor logs is a challenging research field due to three facts: a) traditional two-dimensional maps do not support multiple events to occur at the same spatial location, b) three-dimensional solutions introduce ambiguity and are hard to navigate, and c) map distortions to solve the overlap problem are unfamiliar to most users. This paper introduces a novel approach to represent spatial data changing over time by plotting a number of non-overlapping pixels, close to the sensor positions in a map. Thereby, we encode the amount of time that a subject spent at a particular sensor to the number of plotted pixels. Color is used in a twofold manner; while distinct colors distinguish between sensor nodes in different regions, the colors' intensity is used as an indicator to the temporal property of the subjects' activity. The resulting visualization technique, called growth ring maps, enables users to find similarities and extract patterns of interest in spatiotemporal data by using humans' perceptual abilities. We demonstrate the newly introduced technique on a dataset that shows the behavior of healthy and Alzheimer transgenic, male and female mice. We motivate the new technique by showing that the temporal analysis based on hierarchical clustering and the spatial analysis based on transition matrices only reveal limited results. Results and findings are cross-validated using multidimensional scaling. While the focus of this paper is to apply our visualization for monitoring animal behavior, the technique is also applicable for analyzing data, such as packet tracing, geographic monitoring of sales development, or mobile phone capacity planning.},
}
@misc{p884,
  year = 2009,
  title = {Analysis of community-contributed space- and time-referenced data (example of flickr and panoramio photos)},
  doi = {10.1109/VAST.2009.5333472},
  url = {http://dx.doi.org/10.1109/VAST.2009.5333472},
  author = {Andrienko, G. and Andrienko, N. and Bak, P. and Kisilevich, S. and Keim, D.A.},
  pages = {213--214},
  keywords = {},
  abstract = {Space- and time-referenced data published on the Web by general people can be viewed in a dual way: as independent spatio-temporal events and as trajectories of people in the geographical space. These two views suppose different approaches to the analysis, which can yield different kinds of valuable knowledge about places and about people. We define possible types of analysis tasks related to the two views of the data and present several analysis methods appropriate for these tasks. The methods are suited to large amounts of the data.},
}
@inproceedings{p889,
  booktitle = {Proc. VAST},
  year = 2009,
  title = {Combining automated analysis and visualization techniques for effective exploration of high-dimensional data},
  doi = {10.1109/VAST.2009.5332628},
  url = {http://dx.doi.org/10.1109/VAST.2009.5332628},
  author = {Tatu, A. and Albuquerque, G. and Eisemann, M. and Schneidewind, J. and Theisel, H. and Magnor, M. and Keim, D.A.},
  pages = {59--66},
  keywords = {\n},
  abstract = {Visual exploration of multivariate data typically requires projection onto lower-dimensional representations. The number of possible representations grows rapidly with the number of dimensions, and manual exploration quickly becomes ineffective or even unfeasible. This paper proposes automatic analysis methods to extract potentially relevant visual structures from a set of candidate visualizations. Based on features, the visualizations are ranked in accordance with a specified user task. The user is provided with a manageable number of potentially useful candidate visualizations, which can be used as a starting point for interactive data analysis. This can effectively ease the task of finding truly useful visualizations and potentially speed up the data exploration task. In this paper, we present ranking measures for class-based as well as non class-based Scatterplots and Parallel Coordinates visualizations. The proposed analysis methods are evaluated on different datasets.},
}
@misc{p902,
  year = 2009,
  title = {Integrative visual analytics for suspicious behavior detection},
  doi = {10.1109/VAST.2009.5334430},
  url = {http://dx.doi.org/10.1109/VAST.2009.5334430},
  author = {Bak, P. and Rohrdantz, C. and Leifert, S. and Granacher, C. and Koch, S. and Butscher, S. and Jungk, P. and Keim, D.A.},
  keywords = {},
  abstract = {In the VAST Challenge 2009 suspicious behavior had to be detected applying visual analytics to heterogeneous data, such as network traffic, social network enriched with geo-spatial attributes, and finally video surveillance data. This paper describes some of the awarded parts from our solution entry.},
}
@misc{p916,
  year = 2009,
  title = {Poster: Visual prediction of time series},
  doi = {10.1109/VAST.2009.5333420},
  url = {http://dx.doi.org/10.1109/VAST.2009.5333420},
  author = {Hao, M.C. and Janetzko, H. and Sharma, R. and Dayal, U. and Keim, D.A. and Castellanos, M.},
  pages = {229--230},
  keywords = {},
  abstract = {Many well-known time series prediction methods have been used daily by analysts making decisions. To reach a good prediction, we introduce several new visual analysis techniques of smoothing, multi-scaling, and weighted average with the involvement of human expert knowledge. We combine them into a well-fitted method to perform prediction. We have applied this approach to predict resource consumption in data center for next day planning.},
}
@inproceedings{p933,
  booktitle = {Proc. VAST},
  year = 2009,
  title = {Visual opinion analysis of customer feedback data},
  doi = {10.1109/VAST.2009.5333919},
  url = {http://dx.doi.org/10.1109/VAST.2009.5333919},
  author = {Oelke, D. and Ming Hao and Rohrdantz, C. and Keim, D.A. and Dayal, U. and Haug, L. and Janetzko, H.},
  pages = {187--194},
  keywords = {Visual Opinion Analysis, Visual Sentiment Analysis, Visual Document Analysis, Attribute Extraction},
  abstract = {Today, online stores collect a lot of customer feedback in the form of surveys, reviews, and comments. This feedback is categorized and in some cases responded to, but in general it is underutilized - even though customer satisfaction is essential to the success of their business. In this paper, we introduce several new techniques to interactively analyze customer comments and ratings to determine the positive and negative opinions expressed by the customers. First, we introduce a new discrimination-based technique to automatically extract the terms that are the subject of the positive or negative opinion (such as price or customer service) and that are frequently commented on. Second, we derive a Reverse-Distance-Weighting method to map the attributes to the related positive and negative opinions in the text. Third, the resulting high-dimensional feature vectors are visualized in a new summary representation that provides a quick overview. We also cluster the reviews according to the similarity of the comments. Special thumbnails are used to provide insight into the composition of the clusters and their relationship. In addition, an interactive circular correlation map is provided to allow analysts to detect the relationships of the comments to other important attributes and the scores. We have applied these techniques to customer comments from real-world online stores and product reviews from web sites to identify the strength and problems of different products and services, and show the potential of our technique.},
}
@inproceedings{p1064,
  booktitle = {Proc. VAST},
  year = 2008,
  title = {Visual evaluation of text features for document summarization and analysis},
  doi = {10.1109/VAST.2008.4677359},
  url = {http://dx.doi.org/10.1109/VAST.2008.4677359},
  author = {Oelke, D. and Bak, P. and Keim, D.A. and Last, M. and Danon, G.},
  pages = {75--82},
  keywords = {},
  abstract = {Thanks to the Web-related and other advanced technologies, textual information is increasingly being stored in digital form and posted online. Automatic methods to analyze such textual information are becoming inevitable. Many of those methods are based on quantitative text features. Analysts face the challenge to choose the most appropriate features for their tasks. This requires effective approaches for evaluation and feature-engineering.},
}
@article{p1138,
  journal = {IEEE TVCG},
  year = 2007,
  title = {Visual Analysis of Network Traffic for Resource Planning, Interactive Monitoring, and Interpretation of Security Threats},
  doi = {10.1109/TVCG.2007.70522},
  url = {http://dx.doi.org/10.1109/TVCG.2007.70522},
  author = {Mansmann, F. and Keim, D.A. and North, S.C. and Rexroad, B. and Sheleheda, D.},
  pages = {1105--1112},
  keywords = {Information visualization, network security, network monitoring, treemap},
  abstract = {The Internet has become a wild place: malicious code is spread on personal computers across the world, deploying botnets ready to attack the network infrastructure. The vast number of security incidents and other anomalies overwhelms attempts at manual analysis, especially when monitoring service provider backbone links. We present an approach to interactive visualization with a case study indicating that interactive visualization can be applied to gain more insight into these large data sets. We superimpose a hierarchy on IP address space, and study the suitability of Treemap variants for each hierarchy level. Because viewing the whole IP hierarchy at once is not practical for most tasks, we evaluate layout stability when eliding large parts of the hierarchy, while maintaining the visibility and ordering of the data of interest.},
}
@inproceedings{p1158,
  booktitle = {Proc. VAST},
  year = 2007,
  title = {Intelligent Visual Analytics Queries},
  doi = {10.1109/VAST.2007.4389001},
  url = {http://dx.doi.org/10.1109/VAST.2007.4389001},
  author = {Hao, M.C. and Dayal, U. and Keim, D.A. and Morent, D. and Schneidewind, J.},
  pages = {91--98},
  keywords = {Visual Analytics Query, Similarity Queries, Interactive Queries},
  abstract = {Visualizations of large multi-dimensional data sets, occurring in scientific and commercial applications, often reveal interesting local patterns. Analysts want to identify the causes and impacts of these interesting areas, and they also want to search for similar patterns occurring elsewhere in the data set. In this paper we introduce the Intelligent Visual Analytics Query (IVQuery) concept that combines visual interaction with automated analytical methods to support analysts in discovering the special properties and relations of identified patterns. The idea of IVQuery is to interactively select focus areas in the visualization. Then, according to the characteristics of the selected areas, such as the data dimensions and records, IVQuery employs analytical methods to identify the relationships to other portions of the data set. Finally, IVQuery generates visual representations for analysts to view and refine the results. IVQuery has been applied successfully to different real-world data sets, such as data warehouse performance, product sales, and sever performance analysis, and demonstrates the benefits of this technique over traditional filtering and zooming techniques. The visual analytics query technique can be used with many different types of visual representation. In this paper we show how to use IVQuery with parallel coordinates, visual maps, and scatter plots.},
}
@inproceedings{p1162,
  booktitle = {Proc. VAST},
  year = 2007,
  title = {Literature Fingerprinting: A New Method for Visual Literary Analysis},
  doi = {10.1109/VAST.2007.4389004},
  url = {http://dx.doi.org/10.1109/VAST.2007.4389004},
  author = {Keim, D.A. and Oelke, D.},
  pages = {115--122},
  keywords = {Visual literature analysis, visual analytics, literature fingerprinting},
  abstract = {In computer-based literary analysis different types of features are used to characterize a text. Usually, only a single feature value or vector is calculated for the whole text. In this paper, we combine automatic literature analysis methods with an effective visualization technique to analyze the behavior of the feature values across the text. For an interactive visual analysis, we calculate a sequence of feature values per text and present them to the user as a characteristic fingerprint. The feature values may be calculated on different hierarchy levels, allowing the analysis to be done on different resolution levels. A case study shows several successful applications of our new method to known literature problems and demonstrates the advantage of our new visual literature fingerprinting.},
}
@misc{p1164,
  year = 2007,
  title = {Outlook for Visual Analytics Research Funding},
  doi = {10.1109/VAST.2007.4389030},
  url = {http://dx.doi.org/10.1109/VAST.2007.4389030},
  author = {Thomas, J. and Keim, D.A. and Kielman, J. and Rosenblum, L.},
  pages = {227--227},
  keywords = {},
  abstract = {Visual Analytics has become a rapidly growing field of study. It is also a field that is addressing very significant real world problems in homeland security, business analytics, emergency management, genetics and bioinformatics, investigative analysis, medical analytics, and other areas. For both these reasons, it is attracting new funding and will continue to do so in the future. Visual analytics has also become an international field, with significant research efforts in Canada, Europe, and Australia, as well as the U.S. There is significant new research funding in Canada and Germany with other efforts being discussed, including a major program sponsored by the European Union. The contributors to this panel are some of the primary thought leaders providing research funding or involved in setting up the funding apparatus. We have asked them to present their needs, funding programs, and expectations from the research community. They all come from different perspectives, different missions, and different expectations. They will present their views of the range of activity in both the U.S. and internationally and discuss what is coming. Come learn about these programs, initiatives, and plans, and how you can contribute.},
}
@article{p1270,
  journal = {IEEE TVCG},
  year = 2006,
  title = {Visualization of Geo-spatial Point Sets via Global Shape Transformation and Local Pixel Placement},
  doi = {10.1109/TVCG.2006.198},
  url = {http://dx.doi.org/10.1109/TVCG.2006.198},
  author = {Panse, C. and Sips, M. and Keim, D.A. and North, S.C.},
  pages = {749--756},
  keywords = {Geo-spatial Data, Shape Transformation, Cartogram, Pixel Visualization},
  abstract = {In many applications, data is collected and indexed by geo-spatial location. Discovering interesting patterns through visualization is an important way of gaining insight about such data. A previously proposed approach is to apply local placement functions such as PixelMaps that transform the input data set into a solution set that preserves certain constraints while making interesting patterns more obvious and avoid data loss from overplotting. In experience, this family of spatial transformations can reveal fine structures in large point sets, but it is sometimes difficult to relate those structures to basic geographic features such as cities and regional boundaries. Recent information visualization research has addressed other types of transformation functions that make spatially-transformed maps with recognizable shapes. These types of spatial-transformation are called global shape functions. In particular, cartogram-based map distortion has been studied. On the other hand, cartogram-based distortion does not handle point sets readily. In this study, we present a framework that allows the user to specify a global shape function and a local placement function. We combine cartogram-based layout (global shape) with PixelMaps (local placement), obtaining some of the benefits of each toward improved exploration of dense geo-spatial data sets},
}
@inproceedings{p1298,
  booktitle = {Proc. VAST},
  year = 2006,
  title = {Monitoring Network Traffic with Radial Traffic Analyzer},
  doi = {10.1109/VAST.2006.261438},
  url = {http://dx.doi.org/10.1109/VAST.2006.261438},
  author = {Keim, D.A. and Mansmann, F. and Schneidewind, J. and Schreck, T.},
  pages = {123--128},
  keywords = {Visual Analytics, Network Traffic Monitoring, Information\nVisualization and Geography-based Solutions},
  abstract = {Extensive spread of malicious code on the Internet and also within intranets has risen the user's concern about what kind of data is transferred between her or his computer and other hosts on the network. Visual analysis of this kind of information is a challenging task, due to the complexity and volume of the data type considered, and requires special design of appropriate visualization techniques. In this paper, we present a scalable visualization toolkit for analyzing network activity of computer hosts on a network. The visualization combines network packet volume and type distribution information with geographic information, enabling the analyst to use geographic distortion techniques such as the HistoMap technique to become aware of the traffic components in the course of the analysis. The presented analysis tool is especially useful to compare important network load characteristics in a geographically aware display, to relate communication partners, and to identify the type of network traffic occurring. The results of the analysis are helpful in understanding typical network communication activities, and in anticipating potential performance bottlenecks or problems. It is suited for both off-line analysis of historic data, and via animation for on-line monitoring of packet-based network traffic in real time},
}
@inproceedings{p1304,
  booktitle = {Proc. VAST},
  year = 2006,
  title = {Pixnostics: Towards Measuring the Value of Visualization},
  doi = {10.1109/VAST.2006.261423},
  url = {http://dx.doi.org/10.1109/VAST.2006.261423},
  author = {Schneidewind, J. and Sips, M. and Keim, D.A.},
  pages = {199--206},
  keywords = {Visual Data Exploration, Visualization technique,\nVisual Analytics},
  abstract = {During the last two decades a wide variety of advanced methods for the visual exploration of large data sets have been proposed. For most of these techniques user interaction has become a crucial element, since there are many situations in which a user or an analyst has to select the right parameter settings from among many or select a subset of the available attribute space for the visualization process, in order to construct valuable visualizations that provide insight, into the data and reveal interesting patterns. The right choice of input parameters is often essential, since suboptimal parameter settings or the investigation of irrelevant data dimensions make the exploration process more time consuming and may result in wrong conclusions. In this paper we propose a novel method for automatically determining meaningful parameter- and attribute settings based on the information content of the resulting visualizations. Our technique called Pixnostics, in analogy to Scagnostics (Wilkinson et al., 2005), automatically analyses pixel images resulting from diverse parameter mappings and ranks them according to the potential value for the user. This allows a more effective and more efficient visual data analysis process, since the attribute/parameter space is reduced to meaningful selections and thus the analyst obtains faster insight into the data. Real world applications are provided to show the benefit of the proposed approach},
}
@inproceedings{p1400,
  booktitle = {Proc. InfoVis},
  year = 2005,
  title = {Importance-driven visualization layouts for large time series data},
  doi = {10.1109/INFVIS.2005.1532148},
  url = {http://dx.doi.org/10.1109/INFVIS.2005.1532148},
  author = {Hao, M.C. and Dayal, U. and Keim, D.A. and Schreck, T.},
  pages = {203--210},
  keywords = {Information Visualization, Time Series, Space-Filling Layout Generation},
  abstract = {Time series are an important type of data with applications in virtually every aspect of the real world. Often a large number of time series have to be monitored and analyzed in parallel. Sets of time series may show intrinsic hierarchical relationships and varying degrees of importance among the individual time series. Effective techniques for visually analyzing large sets of time series should encode the relative importance and hierarchical ordering of the time series data by size and position, and should also provide a high degree of regularity in order to support comparability by the analyst. In this paper, we present a framework for visualizing large sets of time series. Based on the notion of inter time series importance relationships, we define a set of objective functions that space-filling layout schemes for time series data should obey. We develop an efficient algorithm addressing the identified problems by generating layouts that reflect hierarchy and importance based relationships in a regular layout with favorable aspect ratios. We apply our technique to a number of real world data sets including sales and stock data, and we compare our technique with an aspect ratio aware variant of the well known TreeMap algorithm. The examples show the advantages and practical usefulness of our layout algorithm.},
}
@misc{p1527,
  year = 2004,
  title = {Exploring and Visualizing the History of InfoVis},
  doi = {10.1109/INFVIS.2004.22},
  url = {http://dx.doi.org/10.1109/INFVIS.2004.22},
  author = {Keim, D.A. and Barro, H. and Panse, C. and Schneidewind, J. and Sips, M.},
  pages = {r6--r6},
  keywords = {},
  abstract = {},
}
@misc{p1538,
  year = 2004,
  title = {Interactive Poster: Visual Mining of Business Process Data},
  doi = {10.1109/INFVIS.2004.41},
  url = {http://dx.doi.org/10.1109/INFVIS.2004.41},
  author = {Hao, M.C. and Keim, D.A. and Dayal, U. and Schneidewind, J.},
  pages = {10--10},
  keywords = {},
  abstract = {},
}
@inproceedings{p1551,
  booktitle = {Proc. InfoVis},
  year = 2004,
  title = {RecMap: Rectangular Map Approximations},
  doi = {10.1109/INFVIS.2004.57},
  url = {http://dx.doi.org/10.1109/INFVIS.2004.57},
  author = {Heilmann, R. and Keim, D.A. and Panse, C. and Sips, M.},
  pages = {33--40},
  keywords = {Geographic Visualization, Information Visualization, Database and Data Mining Visualization},
  abstract = {In many application domains, data is collected and referenced by its geospatial location. Nowadays, different kinds of maps are used to emphasize the spatial distribution of one or more geospatial attributes. The nature of geospatial statistical data is the highly nonuniform distribution in the real world data sets. This has several impacts on the resulting map visualizations. Classical area maps tend to highlight patterns in large areas, which may, however, be of low importance. Cartographers and geographers used cartograms or value-by-area maps to address this problem long before computers were available. Although many automatic techniques have been developed, most of the value-by-area cartograms are generated manually via human interaction. In this paper, we propose a novel visualization technique for geospatial data sets called RecMap. Our technique approximates a rectangular partition of the (rectangular) display area into a number of map regions preserving important geospatial constraints. It is a fully automatic technique with explicit user control over all exploration constraints within the exploration process. Experiments show that our technique produces visualizations of geospatial data sets, which enhance the discovery of global and local correlations, and demonstrate its performance in a variety of applications},
}
@misc{p1570,
  year = 2004,
  title = {2D Maps for Visual Analysis and Retrieval in Large Multi-Feature 3D Model Databases},
  doi = {10.1109/VISUAL.2004.2},
  url = {http://dx.doi.org/10.1109/VISUAL.2004.2},
  author = {Bustos, B. and Keim, D.A. and Panse, C. and Schreck, T.},
  pages = {2--2},
  keywords = {},
  abstract = {Multimedia objects are often described by high-dimensional feature vectors which can be used for retrieval and clustering tasks. We have built an interactive retrieval system for 3D model databases that implements a variety of different feature transforms. Recently, we have enhanced the functionality of our system by integrating a SOM-based visualization module. In this poster demo, we show how 2D maps can be used to improve the effectiveness of retrieval, clustering, and over-viewing tasks in a 3D multimedia system.},
}
@misc{p1661,
  year = 2004,
  title = {VisBiz: A Simplified Visualization of Business Operation},
  doi = {10.1109/VISUAL.2004.109},
  url = {http://dx.doi.org/10.1109/VISUAL.2004.109},
  author = {Hao, M.C. and Keim, D.A. and Dayal, U.},
  pages = {1--1},
  keywords = {},
  abstract = {},
}
@inproceedings{p1805,
  booktitle = {Proc. InfoVis},
  year = 2002,
  title = {Efficient cartogram generation: a comparison},
  doi = {10.1109/INFVIS.2002.1173144},
  url = {http://dx.doi.org/10.1109/INFVIS.2002.1173144},
  author = {Keim, D.A. and North, S.C. and Panse, C. and Schneidewind, J.},
  pages = {33--36},
  keywords = {},
  abstract = {Cartograms are a well-known technique for showing geography-related statistical information, such as population demographics and epidemiological data. The basic idea is to distort a map by resizing its regions according to a statistical parameter, but in a way that keeps the map recognizable. We deal with the problem of making continuous cartograms that strictly retain the topology of the input mesh. We compare two algorithms to solve the continuous cartogram problem. The first one uses an iterative relocation of the vertices based on scanlines. The second one is based on the Gridfit technique, which uses pixel-based distortion based on a quadtree-like data structure.},
}
@inproceedings{p1916,
  booktitle = {Proc. InfoVis},
  year = 2001,
  title = {Pixel bar charts: a new technique for visualizing large multi-attribute data sets without aggregation},
  doi = {10.1109/INFVIS.2001.963288},
  url = {http://dx.doi.org/10.1109/INFVIS.2001.963288},
  author = {Keim, D.A. and Hao, M.C. and Ladisch, J. and Hsu, M. and Dayal, U.},
  pages = {113--120},
  keywords = {},
  abstract = {Simple presentation graphics are intuitive and easy-to-use, but show only highly aggregated data and present only a very limited number of data values (as in the case of bar charts). In addition, these graphics may have a high degree of overlap which may occlude a significant portion of the data values (as in the case of the x-y plots). In this paper, we therefore propose a generalization of traditional bar charts and x-y-plots which allows the visualization of large amounts of data. The basic idea is to use the pixels within the bars to present the detailed information of the data records. Our so-called pixel bar charts retain the intuitiveness of traditional bar charts while allowing very large data sets to be visualized in an effective way. We show that, for an effective pixel placement, we have to solve complex optimization problems, and present an algorithm which efficiently solves the problem. Our application using real-world e-commerce data shows the wide applicability and usefulness of our new idea.},
}
@inproceedings{p2187,
  booktitle = {Proc. Vis},
  year = 1999,
  title = {Visualizing large-scale telecommunication networks and services},
  doi = {10.1109/VISUAL.1999.809930},
  url = {http://dx.doi.org/10.1109/VISUAL.1999.809930},
  author = {Koutsofios, E.E. and North, S.C. and Truscott, R. and Keim, D.A.},
  pages = {457--461},
  keywords = {},
  abstract = {Visual exploration of massive datasets arising from telecommunication networks and services is a challenge. This paper describes SWIFT-3D, an integrated data visualization and exploration system created at AT&T Labs for large scale network analysis. SWIFT-3D integrates a collection of interactive tools that includes pixel-oriented 2D maps, interactive 3D maps, statistical displays, network topology diagrams and an interactive drill-down query interface. Example applications are described, demonstrating a successful application to analyze unexpected network events (high volumes of unanswered calls), and comparison of usage of an Internet service with voice network traffic and local access coverage.},
}
@inproceedings{p2208,
  booktitle = {Proc. InfoVis},
  year = 1998,
  title = {Similarity clustering of dimensions for an enhanced visualization of multidimensional data},
  doi = {10.1109/INFVIS.1998.729559},
  url = {http://dx.doi.org/10.1109/INFVIS.1998.729559},
  author = {Ankerst, M. and Berchtold, S. and Keim, D.A.},
  pages = {52--60, 153},
  keywords = {},
  abstract = {The order and arrangement of dimensions (variates) is crucial for the effectiveness of a large number of visualization techniques such as parallel coordinates, scatterplots, recursive pattern, and many others. We describe a systematic approach to arrange the dimensions according to their similarity. The basic idea is to rearrange the data dimensions such that dimensions showing a similar behavior are positioned next to each other. For the similarity clustering of dimensions, we need to define similarity measures which determine the partial or global similarity of dimensions. We then consider the problem of finding an optimal one- or two-dimensional arrangement of the dimensions based on their similarity. Theoretical considerations show that both, the one- and the two-dimensional arrangement problem are surprisingly hard problems, i.e. they are NP complete. Our solution of the problem is therefore based on heuristic algorithms. An empirical evaluation using a number of different visualization techniques shows the high impact of our similarity clustering of dimensions on the visualization results},
}
@inproceedings{p2276,
  booktitle = {Proc. Vis},
  year = 1998,
  title = {The Gridfit algorithm: an efficient and effective approach to visualizing large amounts of spatial data},
  doi = {10.1109/VISUAL.1998.745301},
  url = {http://dx.doi.org/10.1109/VISUAL.1998.745301},
  author = {Keim, D.A. and Herrmann, A.},
  pages = {181--188},
  keywords = {visualizing large data sets, visualizing spatially referenced data, visualizing geographical data, interfaces to databases},
  abstract = {In a large number of applications, data is collected and referenced by their spatial locations. Visualizing large amounts of spatially referenced data on a limited-size screen display often results in poor visualizations due to the high degree of overplotting of neighboring datapoints. We introduce a new approach to visualizing large amounts of spatially referenced data. The basic idea is to intelligently use the unoccupied pixels of the display instead of overplotting data points. After formally describing the problem, we present two solutions which are based on: placing overlapping data points on the nearest unoccupied pixel; and shifting data points along a screen-filling curve (e.g., Hilbert-curve). We then develop a more sophisticated approach called Gridfit, which is based on a hierarchical partitioning of the data space. We evaluate all three approaches with respect to their efficiency and effectiveness and show the superiority of the Gridfit approach. For measuring the effectiveness, we not only present the resulting visualizations but also introduce mathematical effectiveness criteria measuring properties of the generated visualizations with respect to the original data such as distance- and position-preservation.},
}
@inproceedings{p2529,
  booktitle = {Proc. Vis},
  year = 1995,
  title = {Recursive pattern: a technique for visualizing very large amounts of data},
  doi = {10.1109/VISUAL.1995.485140},
  url = {http://dx.doi.org/10.1109/VISUAL.1995.485140},
  author = {Keim, D.A. and Kriegel, H.-P. and Ankerst, M.},
  pages = {279--286, 463},
  keywords = {Visualizing Large Data Sets, Visualizing Multidimensional and Multivariate Data, Visualizing Large Sequential Data Sets, Recursive Visualization Techniques, Interfaces to Databases},
  abstract = {An important goal of visualization technology is to support the exploration and analysis of very large amounts of data. In this paper, we propose a new visualization technique called a `recursive pattern', which has been developed for visualizing large amounts of multidimensional data. The technique is based on a generic recursive scheme which generalizes a wide range of pixel-oriented arrangements for displaying large data sets. By instantiating the technique with adequate data- and application-dependent parameters, the user may greatly influence the structure of the resulting visualizations. Since the technique uses one pixel for presenting each data value, the amount of data which can be displayed is only limited by the resolution of current display technology and by the limitations of human perceptibility. Beside describing the basic idea of the `recursive pattern' technique, we provide several examples of useful parameter settings for the various recursion levels. We further show that our `recursive pattern' technique is particularly advantageous for the large class of data sets which have a natural order according to one dimension (e.g. time series data). We demonstrate the usefulness of our technique by using a stock market application},
}
@inproceedings{p2650,
  booktitle = {Proc. Vis},
  year = 1993,
  title = {Visual feedback in querying large databases},
  doi = {10.1109/VISUAL.1993.398864},
  url = {http://dx.doi.org/10.1109/VISUAL.1993.398864},
  author = {Keim, D.A. and Kriegel, H.-P. and Seidl, T.},
  pages = {158--165},
  keywords = {},
  abstract = {In this paper, we describe a database query system that provides visual relevance feedback in querying large databases. The goal of our system is to support the query specification process by using each pixel of the display to represent one data item of the database. By arranging and coloring the pixels according to their relevance for the query, the user gets a visual impression of the resulting data set. Using sliders for each condition of the query, the user may change the query dynamically and receives immediate feedback by the visual representation of the resulting data set. By using multiple windows for different parts of a complex query, the user gets visual feedback for each part of the query and, therefore, will easier understand the overall result. The system may be used to query any database that contains tens of thousands to millions of data items, but it is especially helpful to explore large data sets with an unknown distribution of values and to find the interesting hot spots in huge amounts of data. The direct feedback allows to visually display the influence of incremental query refinements and, therefore, allows a better, easier and faster query specification},
}
@inproceedings{p2699,
  booktitle = {Proc. Vis},
  year = 1992,
  title = {Visual query specification in a multimedia database system},
  doi = {10.1109/VISUAL.1992.235208},
  url = {http://dx.doi.org/10.1109/VISUAL.1992.235208},
  author = {Keim, D.A. and Lum, V.},
  pages = {194--201},
  keywords = {Visual Query Specijication, Graphical User Interface, Multimedia Database System, Natural-Language Interface, Information Retrieval, Image Data Management},
  abstract = {A visual interface for a multimedia database management system (MDBMS) is described. DBMS query languages are linear in syntax. Although natural language interfaces have been found to be useful, natural language is ambiguous and difficult to process. For queries on standard (relational) data, these difficulties can be avoided with the use of a visual, graphical interface to guide the user in specifying the query. For image and other media data which are ambiguous in nature, natural language processing, combined with direct graphical access to the domain knowledge, is used to interpret and evaluate the natural language query. The system fully supports graphical and image input/output in different formats. The combination of visual effect and natural language specification, the support of media data, and the allowance of incremental query specification simplify the process of query specification not only for image or multimedia databases but also for all databases},
}
